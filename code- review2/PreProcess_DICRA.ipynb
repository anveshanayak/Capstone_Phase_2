{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import pandas as pd\n",
    "from shapely.geometry import box\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "from shapely.geometry import shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"C:\\Anna_Data_D_files\\sem6\\z_capstone\\Agriculture\\A_Phase_1\\datasets\\DiCRA\\Telengana\"\n",
    "\n",
    "# Lists for the types of factors, years, and specific factors\n",
    "types_of_factors = ['Environmental']  # Add more as needed\n",
    "years = ['2023', '2022']  # Add more years as needed\n",
    "factors = {\n",
    "    'Environmental': ['SSM', 'LST', 'TEMPERATURE','PRECIPITATION', 'NO2', 'PM25'] # Add more as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_centroids_from_geojson(filepath):\n",
    "    data_list = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        geojson = json.load(file)\n",
    "\n",
    "        # Loop through each feature\n",
    "        for feature in geojson.get('features', []):\n",
    "            properties = feature.get('properties', {})\n",
    "            centroid = properties.get('centroid', [None, None])  # Extract centroid\n",
    "            # district_name= properties.get(district_name, None)\n",
    "            # Prepare a dictionary of properties including the centroid\n",
    "            data = {\n",
    "                'district_name': properties.get('district_name', None),  # Extract the name_key property\n",
    "                'centroid': centroid\n",
    "            }\n",
    "            data_list.append(data)\n",
    "    return pd.DataFrame(data_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_vector(vector_files, comprehensive_df):\n",
    "#     # Process vector files\n",
    "#     for vector_file in vector_files:\n",
    "#         # Only process files in the 'DISTRICT' folder\n",
    "#         if 'DISTRICT' in vector_file:\n",
    "#             gdf = gpd.read_file(vector_file)\n",
    "#             gdf = gdf.to_crs(epsg=4326)  # Convert to WGS 84 if necessary\n",
    "\n",
    "#             # Extract the zonal statistics from the 'zonalstat' column (which is in dictionary format)\n",
    "#             zonalstats = gdf['zonalstat'].apply(pd.Series)  # Convert dictionary to separate columns\n",
    "\n",
    "#             # Combine zonal statistics with the rest of the DataFrame\n",
    "#             vector_data = gdf.drop(columns=['geometry', 'zonalstat'])  # Drop 'geometry' and 'zonalstat' for now\n",
    "#             vector_data = pd.concat([vector_data, zonalstats], axis=1)  # Add extracted zonal statistics\n",
    "\n",
    "#             vector_data['geometry'] = gdf['geometry'].apply(lambda geom: geom.bounds)  # Add geometry bounds as a feature\n",
    "#             vector_data['source_file'] = os.path.basename(vector_file)  # Track source file\n",
    "\n",
    "#             # Append to comprehensive DataFrame (update in-place)\n",
    "#             comprehensive_df = pd.concat([comprehensive_df, vector_data], ignore_index=True)\n",
    "    \n",
    "#     return comprehensive_df  # Return updated DataFrame\n",
    "\n",
    "def process_vector(vector_files, comprehensive_df):\n",
    "    # Process vector files\n",
    "    for vector_file in vector_files:\n",
    "        # Only process files in the 'DISTRICT' folder\n",
    "        if 'DISTRICT' in vector_file:\n",
    "            \n",
    "            gdf = gpd.read_file(vector_file)\n",
    "            gdf = gdf.to_crs(epsg=4326)  # Convert to WGS 84 if necessary\n",
    "\n",
    "            # Extract the zonal statistics from the 'zonalstat' column (which is in dictionary format)\n",
    "            zonalstats = gdf['zonalstat'].apply(pd.Series)  # Convert dictionary to separate columns\n",
    "\n",
    "            # Combine zonal statistics with the rest of the DataFrame\n",
    "            vector_data = gdf.drop(columns=['zonalstat'])  # Drop 'geometry' and 'zonalstat' for now\n",
    "            vector_data = pd.concat([vector_data, zonalstats], axis=1)  # Add extracted zonal statistics\n",
    "            if 'centroid' in comprehensive_df.columns:\n",
    "                comprehensive_df = comprehensive_df.drop(columns=['centroid'])\n",
    "            # Extract centroid from the 'properties' of each feature\n",
    "            # vector_data['centroid'] = gdf.apply(lambda row: row['centroid'] if 'centroid' in row else [None, None], axis=1)\n",
    "\n",
    "            vector_data['source_file'] = os.path.basename(vector_file)  # Track source file\n",
    "\n",
    "            # Append to comprehensive DataFrame (update in-place)\n",
    "            comprehensive_df = pd.concat([comprehensive_df, vector_data], ignore_index=True)\n",
    "\n",
    "            centroid_df= extract_centroids_from_geojson(vector_file)\n",
    "            comprehensive_df = comprehensive_df.merge(centroid_df, how='left', on='district_name')\n",
    "    \n",
    "    return comprehensive_df  # Return updated DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raster(raster_files, comprehensive_df):\n",
    "    # Process raster files\n",
    "    for raster_file in raster_files:\n",
    "        with rasterio.open(raster_file) as src:\n",
    "            # Extract metadata and create a bounding box\n",
    "            raster_bounds = src.bounds\n",
    "            raster_crs = src.crs\n",
    "\n",
    "            # Convert raster bounds to a geometry object\n",
    "            bbox_geom = box(*raster_bounds)\n",
    "\n",
    "            # Prepare a DataFrame with just raster metadata\n",
    "            raster_df = pd.DataFrame({\n",
    "                'bounds': [bbox_geom.bounds],  # Bounding box of the raster\n",
    "                'source_file': [os.path.basename(raster_file)]  # Track the source file\n",
    "            })\n",
    "\n",
    "            # Append to comprehensive DataFrame (update in-place)\n",
    "            comprehensive_df = pd.concat([comprehensive_df, raster_df], ignore_index=True)\n",
    "\n",
    "    return comprehensive_df  # Return the updated DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(folder_path):\n",
    "    # Lists to store vector and raster file paths\n",
    "    vector_files = []\n",
    "    raster_files = []\n",
    "    \n",
    "    # Traverse the directory to segregate vector and raster files\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        # print(root, dirs, files)\n",
    "        for file in files:\n",
    "            if file.endswith(('.shp', '.geojson', '.kml')):  # Vector file extensions\n",
    "                vector_files.append(os.path.join(root, file))\n",
    "            elif file.endswith(('.tif', '.tiff')):  # Raster file extensions\n",
    "                raster_files.append(os.path.join(root, file))\n",
    "\n",
    "    comprehensive_df = pd.DataFrame()\n",
    "    if vector_files:\n",
    "        comprehensive_df = process_vector(vector_files, comprehensive_df)\n",
    "    if raster_files:\n",
    "        comprehensive_df = process_raster(raster_files, comprehensive_df)\n",
    "\n",
    "    factor_name = folder_path.split(os.sep)[-1]  # Use os.sep for cross-platform compatibility\n",
    "    factor_name = factor_name + '.csv'\n",
    "    print(factor_name)\n",
    "\n",
    "    r = 'C:\\Anna_Data_D_files\\sem6\\z_capstone\\Agriculture\\A_Phase_2\\code- review2\\preprocessed_files\\DICRA'\n",
    "    comprehensive_df.to_csv(os.path.join(r, factor_name), index=False)\n",
    "    print(f\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSM_2023.csv\n",
      "-----------------------------------------------------\n",
      "LST_2023.csv\n",
      "-----------------------------------------------------\n",
      "TEMPERATURE_2023.csv\n",
      "-----------------------------------------------------\n",
      "PRECIPITATION_2023.csv\n",
      "-----------------------------------------------------\n",
      "NO2_2023.csv\n",
      "-----------------------------------------------------\n",
      "PM25_2023.csv\n",
      "-----------------------------------------------------\n",
      "SSM_2022.csv\n",
      "-----------------------------------------------------\n",
      "LST_2022.csv\n",
      "-----------------------------------------------------\n",
      "TEMPERATURE_2022.csv\n",
      "-----------------------------------------------------\n",
      "PRECIPITATION_2022.csv\n",
      "-----------------------------------------------------\n",
      "NO2_2022.csv\n",
      "-----------------------------------------------------\n",
      "PM25_2022.csv\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def process_files_for_factors(base_folder, types_of_factors, years, factors):\n",
    "    for factor_type in types_of_factors:\n",
    "        for year in years:\n",
    "            for factor in factors.get(factor_type, []):\n",
    "                # Append the year to the factor name\n",
    "                factor_with_year = f\"{factor}_{year}\"\n",
    "                \n",
    "                # Build the directory path\n",
    "                folder_path = os.path.join(base_folder, factor_type, year, factor_with_year)\n",
    "                \n",
    "                # Check if folder exists\n",
    "                if os.path.exists(folder_path):\n",
    "                    # print(folder_path)\n",
    "                    preprocess(folder_path)\n",
    "                else:\n",
    "                    print(f\"Folder not found: {folder_path}\")\n",
    "\n",
    "# Call the function to start processing\n",
    "process_files_for_factors(base_folder, types_of_factors, years, factors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
